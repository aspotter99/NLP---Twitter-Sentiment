{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to do some data cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('../../data/corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.pos_rev.value_counts())\n",
    "print(corpus.neg_rev.value_counts())\n",
    "print(corpus.neutral_or_unknown.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large class imbalance - 2978 positive tweets, 570 negative tweets, 5388 neutral or unknown tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#major class imbalance 2978 positive tweets, 570 negative tweets, 5388 neutral or unknown tweets\n",
    "print(corpus.apple_product.value_counts())\n",
    "print(corpus.google_product.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### also large imbalance between apple (2409) and google (882) products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>pos_rev</th>\n",
       "      <th>neg_rev</th>\n",
       "      <th>neutral_or_unknown</th>\n",
       "      <th>apple_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Great #sxsw ipad app from @madebymany: http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>haha, awesomely rad iPad app by @madebymany ht...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>I just noticed DST is coming this weekend. How...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Just added my #SXSW flights to @planely. Match...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  pos_rev  \\\n",
       "0           0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...        0   \n",
       "1           1  @jessedee Know about @fludapp ? Awesome iPad/i...        1   \n",
       "2           2  @swonderlin Can not wait for #iPad 2 also. The...        1   \n",
       "3           3  @sxsw I hope this year's festival isn't as cra...        0   \n",
       "4           4  Beautifully smart and simple idea RT @madebyma...        1   \n",
       "5           5  Counting down the days to #sxsw plus strong Ca...        1   \n",
       "6           6  Great #sxsw ipad app from @madebymany: http://...        1   \n",
       "7           7  haha, awesomely rad iPad app by @madebymany ht...        1   \n",
       "8           8  I just noticed DST is coming this weekend. How...        0   \n",
       "9           9  Just added my #SXSW flights to @planely. Match...        1   \n",
       "\n",
       "   neg_rev  neutral_or_unknown  apple_product  \n",
       "0        1                   0              1  \n",
       "1        0                   0              1  \n",
       "2        0                   0              1  \n",
       "3        1                   0              1  \n",
       "4        0                   0              1  \n",
       "5        0                   0              1  \n",
       "6        0                   0              1  \n",
       "7        0                   0              1  \n",
       "8        1                   0              1  \n",
       "9        0                   0              1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at apple and google products separately\n",
    "\n",
    "apple = pd.read_csv('../../data/apple.csv')\n",
    "apple.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1949\n",
      "0     460\n",
      "Name: pos_rev, dtype: int64\n",
      "0    2021\n",
      "1     388\n",
      "Name: neg_rev, dtype: int64\n",
      "0    2344\n",
      "1      65\n",
      "Name: neutral_or_unknown, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(apple.pos_rev.value_counts())\n",
    "print(apple.neg_rev.value_counts())\n",
    "print(apple.neutral_or_unknown.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large imbalance here too, 1949 positive, 388 negative, 65 neutral or unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>pos_rev</th>\n",
       "      <th>neg_rev</th>\n",
       "      <th>neutral_or_unknown</th>\n",
       "      <th>google_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Find &amp;amp; Start Impromptu Parties at #SXSW Wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Foursquare ups the game, just in time for #SXS...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Gotta love this #SXSW Google Calendar featurin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Okay, this is really it: yay new @Foursquare f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Really enjoying the changes in Gowalla 3.0 for...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>someone started an #austin @PartnerHub group i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>They were right, the @gowalla 3 app on #androi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  pos_rev  \\\n",
       "0           0  @sxtxstate great stuff on Fri #SXSW: Marissa M...        1   \n",
       "1           1  #SXSW is just starting, #CTIA is around the co...        1   \n",
       "2           2  Excited to meet the @samsungmobileus at #sxsw ...        1   \n",
       "3           3  Find &amp; Start Impromptu Parties at #SXSW Wi...        1   \n",
       "4           4  Foursquare ups the game, just in time for #SXS...        1   \n",
       "5           5  Gotta love this #SXSW Google Calendar featurin...        1   \n",
       "6           6  Okay, this is really it: yay new @Foursquare f...        1   \n",
       "7           7  Really enjoying the changes in Gowalla 3.0 for...        1   \n",
       "8           8  someone started an #austin @PartnerHub group i...        1   \n",
       "9           9  They were right, the @gowalla 3 app on #androi...        1   \n",
       "\n",
       "   neg_rev  neutral_or_unknown  google_product  \n",
       "0        0                   0               1  \n",
       "1        0                   0               1  \n",
       "2        0                   0               1  \n",
       "3        0                   0               1  \n",
       "4        0                   0               1  \n",
       "5        0                   0               1  \n",
       "6        0                   0               1  \n",
       "7        0                   0               1  \n",
       "8        0                   0               1  \n",
       "9        0                   0               1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at google\n",
    "\n",
    "google = pd.read_csv('../../data/google.csv')\n",
    "google.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    723\n",
      "0    159\n",
      "Name: pos_rev, dtype: int64\n",
      "0    751\n",
      "1    131\n",
      "Name: neg_rev, dtype: int64\n",
      "0    856\n",
      "1     26\n",
      "Name: neutral_or_unknown, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(google.pos_rev.value_counts())\n",
    "print(google.neg_rev.value_counts())\n",
    "print(google.neutral_or_unknown.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large imbalance here too, 723 positive, 131 negative, 26 neutral or unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9092, 7)\n",
      "(2409, 6)\n",
      "(882, 6)\n"
     ]
    }
   ],
   "source": [
    "print(corpus.shape)\n",
    "print(apple.shape)\n",
    "print(google.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>pos_rev</th>\n",
       "      <th>neg_rev</th>\n",
       "      <th>neutral_or_unknown</th>\n",
       "      <th>apple_product</th>\n",
       "      <th>google_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Attn: All  #SXSW frineds, @mention Register fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Anyone at  #SXSW who bought the new iPad want ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>At #sxsw.  Oooh. RT @mention Google to Launch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>SPIN Play - a new concept in music discovery f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>VatorNews - Google And Apple Force Print Media...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>HootSuite - HootSuite Mobile for #SXSW ~ Updat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Hey #SXSW - How long do you think it takes us ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  pos_rev  \\\n",
       "0           0  @teachntech00 New iPad Apps For #SpeechTherapy...        0   \n",
       "1           1  Holler Gram for iPad on the iTunes App Store -...        0   \n",
       "2           2  Attn: All  #SXSW frineds, @mention Register fo...        0   \n",
       "3           3      Anyone at  #sxsw want to sell their old iPad?        0   \n",
       "4           4  Anyone at  #SXSW who bought the new iPad want ...        0   \n",
       "5           5  At #sxsw.  Oooh. RT @mention Google to Launch ...        0   \n",
       "6           6  SPIN Play - a new concept in music discovery f...        0   \n",
       "7           7  VatorNews - Google And Apple Force Print Media...        0   \n",
       "8           8  HootSuite - HootSuite Mobile for #SXSW ~ Updat...        0   \n",
       "9           9  Hey #SXSW - How long do you think it takes us ...        0   \n",
       "\n",
       "   neg_rev  neutral_or_unknown  apple_product  google_product  \n",
       "0        0                   1              0               0  \n",
       "1        0                   1              0               0  \n",
       "2        0                   1              0               0  \n",
       "3        0                   1              0               0  \n",
       "4        0                   1              0               0  \n",
       "5        0                   1              0               0  \n",
       "6        0                   1              0               0  \n",
       "7        0                   1              0               0  \n",
       "8        0                   1              0               0  \n",
       "9        0                   1              0               0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do we have a lot of tweets that don't identify a product?  Looks like it, but let's see\n",
    "\n",
    "none = pd.read_csv('../../data/no_prod.csv')\n",
    "none.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>pos_rev</th>\n",
       "      <th>neg_rev</th>\n",
       "      <th>neutral_or_unknown</th>\n",
       "      <th>apple_product</th>\n",
       "      <th>google_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Attn: All  #SXSW frineds, @mention Register fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Anyone at  #SXSW who bought the new iPad want ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  pos_rev  \\\n",
       "0           0  @teachntech00 New iPad Apps For #SpeechTherapy...        0   \n",
       "1           1  Holler Gram for iPad on the iTunes App Store -...        0   \n",
       "2           2  Attn: All  #SXSW frineds, @mention Register fo...        0   \n",
       "3           3      Anyone at  #sxsw want to sell their old iPad?        0   \n",
       "4           4  Anyone at  #SXSW who bought the new iPad want ...        0   \n",
       "\n",
       "   neg_rev  neutral_or_unknown  apple_product  google_product  \n",
       "0        0                   1              0               0  \n",
       "1        0                   1              0               0  \n",
       "2        0                   1              0               0  \n",
       "3        0                   1              0               0  \n",
       "4        0                   1              0               0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "none.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5801, 7)\n",
      "1    5297\n",
      "0     504\n",
      "Name: neutral_or_unknown, dtype: int64\n",
      "(9092, 7)\n",
      "1    5388\n",
      "0    3704\n",
      "Name: neutral_or_unknown, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(none.shape)\n",
    "print(none.neutral_or_unknown.value_counts())\n",
    "print(corpus.shape)\n",
    "print(corpus.neutral_or_unknown.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interesting, the entire set has 5388 neutral or unknown and the majority (5297) of those do not have a product listed.  It looks like this dataset was only rating the tweets for negative or positive emotion if there was a product associated with the tweet.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build some quick baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>pos_rev</th>\n",
       "      <th>neg_rev</th>\n",
       "      <th>neutral_or_unknown</th>\n",
       "      <th>apple_product</th>\n",
       "      <th>google_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  pos_rev  \\\n",
       "0           0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...        0   \n",
       "1           1  @jessedee Know about @fludapp ? Awesome iPad/i...        1   \n",
       "2           2  @swonderlin Can not wait for #iPad 2 also. The...        1   \n",
       "3           3  @sxsw I hope this year's festival isn't as cra...        0   \n",
       "4           4  @sxtxstate great stuff on Fri #SXSW: Marissa M...        1   \n",
       "\n",
       "   neg_rev  neutral_or_unknown  apple_product  google_product  \n",
       "0        1                   0              1               0  \n",
       "1        0                   0              1               0  \n",
       "2        0                   0              1               0  \n",
       "3        1                   0              1               0  \n",
       "4        0                   0              0               1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp = corpus\n",
    "corp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def doc_preparer(doc, stop_words=sw):\n",
    "    '''  \n",
    "    :param doc: a document from the corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corp.tweet_text.astype('str')\n",
    "y = corp.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.25)\n",
    "\n",
    "token_docs = [doc_preparer(doc, sw) for doc in X_train]\n",
    "token_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation train-test split to build our best model\n",
    "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train, test_size=.25, random_state=42)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "X_t_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_0 = y_t.value_counts()['Negative emotion']/len(y_t)\n",
    "prior_1 = y_t.value_counts()['Positive emotion']/len(y_t)\n",
    "prior_2 = y_t.value_counts()['No emotion toward brand or product']/len(y_t)\n",
    "prior_3 = y_t.value_counts()['I can\\'t tell']/len(y_t)\n",
    "print(prior_0, prior_1, prior_2, prior_3)\n",
    "print(np.log(prior_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "y_hat = mnb.predict(X_t_vec)\n",
    "print(accuracy_score(y_t, y_hat))\n",
    "print(confusion_matrix(y_t,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_confusion_matrix(cm = np.array(confusion_matrix(y_t,y_hat)), \n",
    "                      normalize    = True,\n",
    "                      target_names = ['Negative', 'Positive', 'Neutral' , 'Unknown'],\n",
    "                      title        = \"Confusion Matrix\")\n",
    "plt.savefig('fsm_confusion_matrix.png') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial model with no additional stop words removed = .81, not too bad!  However, it appears that what we are seeing here is likely being influenced by the really large class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's remove the 'no emotion' and 'I can't tell' from our data and see what we get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg =  pd.read_sql(\"\"\"\n",
    "                    SELECT tweet_text, is_there_an_emotion_directed_at_a_brand_or_product\n",
    "                    FROM tweets\n",
    "                    WHERE tweet_text != 'None'\n",
    "                    AND is_there_an_emotion_directed_at_a_brand_or_product IN ('Positive emotion','Negative emotion')\n",
    "                    \"\"\",con)\n",
    "pos_neg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pos_neg.tweet_text.astype('str')\n",
    "y = pos_neg.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.25)\n",
    "\n",
    "token_docs = [doc_preparer(doc, sw) for doc in X_train]\n",
    "token_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train, test_size=.25, random_state=42)\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "X_t_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_0 = y_t.value_counts()['Negative emotion']/len(y_t)\n",
    "prior_1 = y_t.value_counts()['Positive emotion']/len(y_t)\n",
    "print(prior_0, prior_1)\n",
    "print(np.log(prior_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = mnb.predict(X_t_vec)\n",
    "print(accuracy_score(y_t, y_hat))\n",
    "print(confusion_matrix(y_t,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm = np.array(confusion_matrix(y_t,y_hat)), \n",
    "                      normalize    = True,\n",
    "                      target_names = ['Negative', 'Positive'],\n",
    "                      title        = \"Confusion Matrix\")\n",
    "plt.savefig('fsm_binomial_confusion_matrix.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is a much better accuracy score (.94), but the confusion matrix shows that the model is best at correctly identifying positive emotion, for our purposes we want the negative emotion tweets to be correctly optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try a word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "pos = pos_neg\n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "for word in pos.tweet_text:\n",
    "    word = str(word)\n",
    "    tokens = word.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, background_color ='white', \n",
    "                      stopwords = stopwords, min_font_size = 10).generate(comment_words) \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too much going on in this to really give us an idea of what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text): #this is Df_pd for Df_np (text[:])\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][:]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_w(pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pos_neg.sample(frac = 0.1, replace = False, random_state=42)\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "# clean text data\n",
    "pos_neg[\"tweet_text\"] = pos_neg[\"tweet_text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets look at polarity = this is a measure on how extremely negative (-1) or positive (1) a tweet is based on the polarity dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "pos_neg['polarity'] = pos_neg['tweet_text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "pos_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random reviews with highest positive polarity:')\n",
    "cl = pos_neg.loc[pos_neg.polarity == 1, ['tweet_text']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])\n",
    "    \n",
    "print('5 random reviews with most negative sentiment:')\n",
    "cl = pos_neg.loc[pos_neg.polarity == -1, ['tweet_text']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Judging from the random sampling of tweets, the polarity score looks to be picking up on emotion pretty well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at the distribution \n",
    "\n",
    "x = pos_neg['polarity']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(x,bins='auto',color = 'blue')\n",
    "plt.xlabel('Polarity Score', fontsize=18)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Polarity Score Frequency Distribution', fontsize = 24):\n",
    "plt.savefig('Polarity_score.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmmmmm, this set did not include any tweets identified as neutral, let's see what the distribution looks like of emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x = pos_neg['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.countplot(x,color = 'blue')\n",
    "plt.xlabel('Emotion', fontsize=18)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Count', fontsize=18)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Pos/neg Sentiment Count', fontsize = 24);\n",
    "plt.savefig('pos/negsentiment.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "odd, none of the tweets were tagged as neutral by the humans reviewing them, but the polarity score indicates a large number of tweets were actually neutral.  Lets take a look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = pos_neg[pos_neg.polarity.eq(0)]\n",
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x = neutral['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.countplot(x,color = 'blue')\n",
    "plt.xlabel('Emotion', fontsize=18)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Count', fontsize=18)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Neutral Sentiment tweets', fontsize = 24);\n",
    "plt.savefig('neutral_sentiment.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, it looks like the 0 polarity score is picking up about the same percentage of pos and neg identified tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_words(pos_neg['tweet_text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "    \n",
    "df = pd.DataFrame(common_words, columns = ['tweet_text' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['tweet_text']\n",
    "y = df['count']\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x,y,color = 'blue')\n",
    "plt.xlabel('Word', fontsize = 30)\n",
    "plt.xticks(rotation=45, fontsize=24)\n",
    "plt.ylabel('Count', fontsize = 30)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.title('Distribution of top 20 Words', fontsize=36);\n",
    "plt.savefig('word_dist.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it probably makes sense to remove most of these.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sw = stopwords.words('english')\n",
    "custom_sw.extend([\"sxsw\",\"mention\",\"link\",\"ipad\",\"rt\",\"apple\",\"google\",\"iphone\",\n",
    "                  \"quot\",\"store\",\"app\",\"new\",\"get\",\"austin\",\"pop\",\"amp\",\"android\",\"launch\",\"go\",\"open\"] )\n",
    "custom_sw[-10:]\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words=custom_sw).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_words(pos_neg['tweet_text'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df1 = pd.DataFrame(common_words, columns = ['tweet_text' , 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df1['tweet_text']\n",
    "y = df1['count']\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x,y,color = 'blue')\n",
    "plt.xlabel('Word', fontsize = 30)\n",
    "plt.xticks(rotation=45, fontsize=24)\n",
    "plt.ylabel('Count', fontsize = 30)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.title('Distribution of top 20 Words', fontsize=36);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "pos = df1\n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "for word in pos.tweet_text:\n",
    "    word = str(word)\n",
    "    tokens = word.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, background_color ='white', \n",
    "                      stopwords = stopwords, min_font_size = 10).generate(comment_words) \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if taking out the additional stop words improved the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 =  pd.read_sql(\"\"\"\n",
    "                    SELECT tweet_text, is_there_an_emotion_directed_at_a_brand_or_product\n",
    "                    FROM tweets\n",
    "                    WHERE tweet_text != 'None'\n",
    "                    AND is_there_an_emotion_directed_at_a_brand_or_product IN ('Positive emotion','Negative emotion')\n",
    "                    \"\"\",con)\n",
    "model_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_2.is_there_an_emotion_directed_at_a_brand_or_product.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def doc_preparer(doc, stop_words=custom_sw):\n",
    "    '''  \n",
    "    :param doc: a document from the corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = model_2.tweet_text.astype('str')\n",
    "y2 = model_2.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "\n",
    "Xm2_train, Xm2_test, ym2_train, ym2_test = train_test_split(X2,y2, random_state=42, test_size=.25)\n",
    "\n",
    "token_docs = [doc_preparer(doc, sw) for doc in X_train]\n",
    "token_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm2_t, Xm2_val, ym2_t, ym2_val = train_test_split(token_docs, ym2_train, test_size=.25, random_state=42)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "Xm2_t_vec = cv.fit_transform(Xm2_t)\n",
    "Xm2_t_vec  = pd.DataFrame.sparse.from_spmatrix(Xm2_t_vec)\n",
    "Xm2_t_vec.columns = sorted(cv.vocabulary_)\n",
    "Xm2_t_vec.set_index(ym2_t.index, inplace=True)\n",
    "Xm2_t_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbm2 = MultinomialNB()\n",
    "mnbm2.fit(Xm2_t_vec, ym2_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_0 = ym2_t.value_counts()['Negative emotion']/len(y_t)\n",
    "prior_1 = ym2_t.value_counts()['Positive emotion']/len(y_t)\n",
    "print(prior_0, prior_1)\n",
    "print(np.log(prior_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ym2_hat = mnbm2.predict(Xm2_t_vec)\n",
    "print(accuracy_score(ym2_t, ym2_hat))\n",
    "print(confusion_matrix(ym2_t,ym2_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## going to try a completely different approach -- starting a new notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amanda",
   "language": "python",
   "name": "amanda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
